import asyncio
import time
from dotenv import load_dotenv
from openai import AsyncOpenAI
from pydantic import BaseModel

# Const Parameters
WAIT_TIME = 3 # (seconds) the time to wait between checking whether the LLM's response is complete
MAX_NUM_WAITS = 1000 # sets wait time for timeout from waiting on model response
VALID_ERROR_TYPES = ["cancelled", "failed", "expired"]
ERROR_CODES = {
  "cancelled": 522,
  "failed": 532,
  "expired": 539
} # TODO: add real codes not just numbers I pulled out of thin air
# TODO: please use 2010s error handling, not 1980s error handling
ERROR_DESCRIPTIONS = {
  "cancelled": "Request was cancelled",
  "failed": "Request failed",
  "expired": "Request expired"
}


# TODO: add validation Annotated to class model
class AssistantSession(BaseModel):
  """
  A class containing all the information to reference a session

  Attributes
  ----------
  assistant_id : str
    ID attribute for an openai assistant object
  thread_id : str
    ID attribute for an openai thread object
  """

  assistant_id: str
  thread_id: str

class AssistantSessionMessage(AssistantSession):
  """
  A class containing a session with a message, inherited from AssistantSession
  
  Extra Attributes
  ----------
  message : str
    A string of the message send to the session
  """
  message: str

# Error

class OpenAIError(Exception):
  """
  A class holding the different types of errors the openai API can raise

  Attributes
  ----------
  error_type : str
    A string holding the name of the error
  error_code : int
    A numerical code for the error
  error_description: str
    A verbal description for the error
  """

  def __init__(self, error_type): # TODO: Encode using something other than dictionaries
    self.error_type = error_type
    self.error_code = ERROR_CODES[self.error_type]
    self.error_description = ERROR_DESCRIPTIONS[self.error_type]

# Initialization commands
load_dotenv()
client = AsyncOpenAI()

async def initialize_conversation():
  """Creates and returns relevant ids for a session
  
  Returns
  -------
  AssistantSession
    IDs for newly created assistant and thread objects
  """
  
  # Create the assistant
  assistant = await client.beta.assistants.create(
    name="Personal Assistant",
    instructions="You are a personal assistant whose role is to help the user complete their tasks and to entertaing the user via conversation.",
    model="gpt-3.5-turbo-1106",
  )

  # Create the thread
  thread = await client.beta.threads.create()

  # Add the ids of the newly created assistand and thread to a new AssistantSession object
  new_session = AssistantSession(assistant_id=assistant.id,thread_id=thread.id)

  return new_session


async def continue_conversation(assistant_message):
  """Takes session information and a new message from the user and returns a response from the LLM
  
  Parameters
  ----------
  current_session : AssistantSession
    IDs for assistant and thread objects of conversation
  next_message : str
    A new message from the user
    
  Returns
  -------
  str
    A response generated by the Asssistant LLM
  """

  # Unpack current_session for ease of access
  threadId = assistant_message.thread_id
  assistantId = assistant_message.assistant_id
  next_message = assistant_message.message

  # Adds the user's message to the thread
  await client.beta.threads.messages.create(
    thread_id=threadId, role="user", content=next_message
  )

  # Makes the LLM start generating its response
  run = await client.beta.threads.runs.create(
    thread_id=threadId, assistant_id=assistantId
  )

  # Waits for the LLM to finish generating its response
  num_waits = 0

  while num_waits < MAX_NUM_WAITS:
    # Check the status of the LLM's response generation
    run = await client.beta.threads.runs.retrieve(
      thread_id=threadId, run_id=run.id
    )

    # Takes action based on the status
    if run.status == "requires_action": # This means that a tool/function needs to be called. Will implement once we have at least one tool. Should probably be implemented as its own function
      continue
    elif run.status in VALID_ERROR_TYPES: # This means there is some sort of error:
      raise OpenAIError(error_type=run.status)
    elif run.status == "completed": # The LLM is done renerating its response
      break
    else:
      num_waits += 1
      time.sleep(WAIT_TIME)
        
  # Grabs the LLM's response from the end of the threatd
  messages = await client.beta.threads.messages.list(thread_id=threadId)
  response = messages.data[0].content[0].text.value

  return response


async def end_conversation(assistant_to_delete):
  """Deletes session information for when the session is over
  
  Parameters
  ----------
  assistant_to_delete : AssistantSession
    IDs of the session to be deleted
  """

  # Attempts to delete the assistant and thread whose ids are contained in the session information and stores the result of the attempt
  thread_deleted = await client.beta.threads.delete(assistant_to_delete.thread_id)
  assistant_deleted = await client.beta.assistants.delete(assistant_to_delete.assistant_id)
  deleted_both = thread_deleted.deleted and assistant_deleted.deleted

  # Returns True iff both are successfully deleted
  return deleted_both
